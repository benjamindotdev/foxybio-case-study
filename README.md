# FoxyBio â€” AI-Powered Bioinformatics Platform

An intelligent web platform that democratizes access to complex bioinformatics analysis by enabling researchers to interact with their data through a natural language chat interface.

<!-- ## Product Overview

![Landing Page](assets/landing.png) -->

## My Role

- Frontend Lead
- Owned the entire frontend architecture and implementation (frontend directory)
- Designed and built the reactive UI for streaming AI responses and real-time data visualization defined API contracts and integration patterns with the backend engineering team
- Refactored legacy patterns into a clean, maintainable, and type-safe codebase

## Problem

- Bioinformatics analysis typically requires specialized coding skills (R/Python) and command-line expertise.
- Biotech researchers often face bottlenecks waiting for bioinformaticians to run standard pipelines.
- Existing tools lack an intuitive interface for exploring complex single-cell sequencing datasets.

## Solution

BioBot bridges the gap between biological data and insight by providing:

- Conversational Interface: A chat-based UI where users can ask questions about their datasets in plain English.
- Visual Feedback: Instant rendering of complex data plots (UMAP, t-SNE) generated by backend R tools.
- Self-Service Prep: A GUI for uploading, validating, and preprocessing raw sequencing data without writing code.

## Architecture & Technical Decisions

- React & Vite: Chosen for high performance and rapid development cycles.
- TypeScript: Strict static typing utilized to match complex backend Java records and ensure data reliability.
- Client-Side Streaming: Implemented robust handling of chunked responses for the AI chat, providing a smooth, "typing" effect for users.
- Docker Integration: Configured proxying and orchestration to run seamlessly alongside a containerized Java/R backend.

## Key decision

Lifting State for Persistence:
- I refactored the application from page-level state to a global Context-based architecture. This ensures that long-running chat sessions and expensive data visualizations persist when users navigate between the Dataset, Profile, and Chat views, preventing data loss and unnecessary API re-fetching.

## Constraints & Trade-offs
- Strict API Security: Had to implement robust error handling and authentication guards to work with a secure, Firebase-backed Java environment.
- Complex Dependency Chain: The frontend needed to be resilient to potential timeouts from the heavy-duty R processing occurring in the background.

## Status

- Core platform modules (Chat, Data Prep, Billing) completed.
- Currently in active development with a focus on optimizing large-dataset visualizations.
- Code repository is private.